---
title: softmax函数
date: '2017-12-14'
linkTitle: /2017/12/14/softmax/
source: Jiaxiang Li's Blog
description: |2-
   深入深出Sigmoid与Softmax的血缘关系 这篇文章，其实解释了怎么从逻辑回归用到的统计模型，过渡到多分类模型Softmax函数。
  其实道理很简单。
  回想逻辑回归的定义。
  \[P(Y=1|X)=\frac{\exp(X\beta)}{1+\exp(X\beta)}\]
  所以，
  \[P(Y=0|X)=1-P(Y=1|X)=1-\frac{\exp(X\beta)}{1+\exp(X\beta)}=\frac{1}{1+\exp(X\beta)}\]
  形象理解为
  \(\exp(X\beta)\)和\(1\)分别为\(P(Y=1|X)\)和\(P(Y=0|X)\)在公式中的权重。
  因此推广， 如果\(Y=1,2,3, ..., j, ..., n\)
  那么有特定的分类。
  \[P(Y=j|X)=\frac{\exp(X \beta^{j})}{\sum_{j=1}^n{\exp(X \beta^{j})}}\]
  这就是Softmax函数的思想。 R中有包，softmaxreg.pdf。 Unsupervised Feature Learning and Deep Learning Tutorial 解释得很好。 ...
disable_comments: true
---
 深入深出Sigmoid与Softmax的血缘关系 这篇文章，其实解释了怎么从逻辑回归用到的统计模型，过渡到多分类模型Softmax函数。
其实道理很简单。
回想逻辑回归的定义。
\[P(Y=1|X)=\frac{\exp(X\beta)}{1+\exp(X\beta)}\]
所以，
\[P(Y=0|X)=1-P(Y=1|X)=1-\frac{\exp(X\beta)}{1+\exp(X\beta)}=\frac{1}{1+\exp(X\beta)}\]
形象理解为
\(\exp(X\beta)\)和\(1\)分别为\(P(Y=1|X)\)和\(P(Y=0|X)\)在公式中的权重。
因此推广， 如果\(Y=1,2,3, ..., j, ..., n\)
那么有特定的分类。
\[P(Y=j|X)=\frac{\exp(X \beta^{j})}{\sum_{j=1}^n{\exp(X \beta^{j})}}\]
这就是Softmax函数的思想。 R中有包，softmaxreg.pdf。 Unsupervised Feature Learning and Deep Learning Tutorial 解释得很好。 ...