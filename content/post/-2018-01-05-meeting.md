---
title: 要讲的地方
date: '2018-01-05'
linkTitle: /2018/01/05/meeting/
source: Jiaxiang Li's Blog
description: |-
  按需增删。
  如果大家需要之后使用xgboost，我的直观理解：
  \[xgboost = boosting + 正则化\]
  整理的正则化考虑了l1、l2、C等，但是实际上都是损失函数的定义，不是特别重点，理论上后期撸代码就好，因此搞懂了boosting，近似于懂了xgboost。 看这里Boosting理论部分 学习笔记 - A Hugo website 。 boosting的前提是决策树，理论部分看这里，决策树理论部分 学习笔记 - A Hugo website 。
  实践中，有一个学习率和梯度下降，这个可以通过吴恩达的这个视频理解，但是里面涉及矩阵运算，单纯理解加撸代码，则不需要太深究，最重要的概念在这。 ( 吴恩达 机器学习导论 梯度下降 学习笔记 - A Hugo website ) 。
  考虑模型KS问题，需要一定的特征工程，常见的NMF理 ...
disable_comments: true
---
按需增删。
如果大家需要之后使用xgboost，我的直观理解：
\[xgboost = boosting + 正则化\]
整理的正则化考虑了l1、l2、C等，但是实际上都是损失函数的定义，不是特别重点，理论上后期撸代码就好，因此搞懂了boosting，近似于懂了xgboost。 看这里Boosting理论部分 学习笔记 - A Hugo website 。 boosting的前提是决策树，理论部分看这里，决策树理论部分 学习笔记 - A Hugo website 。
实践中，有一个学习率和梯度下降，这个可以通过吴恩达的这个视频理解，但是里面涉及矩阵运算，单纯理解加撸代码，则不需要太深究，最重要的概念在这。 ( 吴恩达 机器学习导论 梯度下降 学习笔记 - A Hugo website ) 。
考虑模型KS问题，需要一定的特征工程，常见的NMF理 ...